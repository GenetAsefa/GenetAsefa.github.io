---
title: "Seminar Course on Large Language Model-Enhanced Representation Learning
for Knowledge Graphs (KGs)"
collection: teaching
type: "Seminar"
permalink: /teaching/2025-seminar-rlkg-llm
venue: "AIFB, KIT"
date: 2025-04-30
location: "Karlsruhe, Germany"
---

Title: Seminar Course on Large Language Model-Enhanced Representation Learning
for Knowledge Graphs (KGs)

Effective feature representation is critical for optimizing the performances of machine learning algorithms. Recently, Representation Learning (RL) has advanced significantly, focusing on embedding words and Knowledge Graphs (KGs) into low-dimensional vector spaces. Word embeddings encode words as vectors, capturing context, semantic similarity, and relationships. Similarly, KG representation learning (KGRL) algorithms (a.k.a. KG embedding (KGE) models) are used to represent entities and relations as vectors in a low-dimensional vector space, preserving structure and semantic connections. 

KGE models can be unimodal, using a single source of information, or multimodal, integrating multiple sources such as relations between entities, text literals, numeric literals, images, etc. Capturing information from these sources ensures semantically rich representations. Multimodal KGE models either create separate representations for each source in non-unified spaces or a unified representation for KG elements. These embeddings are commonly used for KG completion tasks such as link prediction and entity classification.

Emerging methodologies for KGRL leverage Large Language Models (LLMs) such as LLaMA, GPT 3.5, and PaLM2. The integration of LLMs with KG KGRL signifies a pivotal advancement in the field of artificial intelligence, enhancing the ability to capture and utilize complex knowledge structures. 

In this seminar, we aim to explore state-of-the-art approaches that utilize LLMs for Knowledge Graph representation learning.
