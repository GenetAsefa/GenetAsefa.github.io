---
title: "Seminar Course on Representation Learning for KGs"
collection: teaching
type: "Seminar"
permalink: /teaching/2021-seminar-kg-rl
venue: "AIFB, KIT"
date: 2021-10-23
location: "Karlsruhe, Germany"
---

Title: Seminar Course on Representation Learning for KGs

Data representation or feature representation plays a key role in the performance of machine learning algorithms. In recent years, rapid growth has been observed in Representation Learning (RL) of words and Knowledge Graphs (KG) into low dimensional vector spaces and its applications to many real-world scenarios. Word embeddings are a low dimensional vector representation of words that are capable of capturing the context of a word in a document, semantic similarity as well as its relation with other words. Similarly, KG embeddings are a low dimensional vector representation of entities and relations from a KG preserving its inherent structure and capturing the semantic similarity between the entities. 

KG representation learning algorithms (a.k.a. KG embedding models) could be either unimodal where a single source is used or multimodal where multiple sources are explored. The sources of information could be relations between entities, text literals, numeric literals, images, etc. It is important to capture the information present in each of these sources in order to learn representations which are rich in semantics.  Multimodal KG embeddings learn either multiple representations simultaneously based on each source of information in a non-unified space or learn a single representation for each element of the KG in a unified space. Representation of entities and relations learnt using both unimodal and multimodal KG  embedding models could be used in various downstream applications such as clustering, classification, and so on. On the other hand, language models such as BERT, ELMo, GPT, etc. learn the probability of word occurrence based on text corpus and learn representation of words in a low-dimensional embedding space. Representation of the words generated by the language models are often used for various KG completion tasks such as link prediction, entity classification, and so on. 

In this seminar, we would like to study the different state of the art algorithms for multimodal embeddings, applications of KG embeddings, or the use of language models for KG representation. 
